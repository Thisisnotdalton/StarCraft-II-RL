{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Getting Started with PySC2</h1>\n",
    "<p>For additional information, please refer to the official GitHub repository linked <a href=\"https://github.com/deepmind/pysc2\">here</a>.</p>\n",
    "<p>To get started with PySC2, first verify that you have the proper Python packages installed as well as the mini game maps linked in the PySC2 repository!</p>\n",
    "<p>Please run the following cell to make sure that your computer has all of the proper software setup.</p>\n",
    "<p>If you have everything installed properly, the running the following cell should open an instance of StarCraft with a randomly acting agent playing on the MoveToBeacon map:</p>\n",
    "<div style=\"background-color:#300a24\"><b><p style=\"color:white\">python3 -m pysc2.bin.agent --map MoveToBeacon --agent pysc2.agents.random_agent.RandomAgent</p></b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Actions</h3>\n",
    "<p>Assuming all of that worked, we will continue by setting up some of the basic configuration for our StarCraft AI. In order to make our AI, we will create a Python class to represent our StarCraft agent. We will make our class inherit from the base agent which PySC2 provides.</p>\n",
    "\n",
    "<p>Some things worth noting are that we define a list of default actions for our agent to use. This will be used to restrict which actions our agent is allowed to perform. To view the list of all valid actions our agent can perform, try entering the following command in a terminal:</p>\n",
    "<div style=\"background-color:#300a24\"><b><p style=\"color:white\">python3 -m pysc2.bin.valid_actions --hide_specific</p></b></div>\n",
    "\n",
    "\n",
    "<p>Running this command produces many lines of output giving you the numerical id's of various StarCraft actions our agent can perform. A smaller subset of actions has been selected with comments with their name so that our agent is not slowed down with too many actions to learn.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pysc2.agents.base_agent import BaseAgent\n",
    "from pysc2.lib import actions, features\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "default_actions = [\n",
    "   0, #no_op                                              ()\n",
    "   1, #move_camera                                        (1/minimap [64, 64])\n",
    "#   2, #select_point                                       (6/select_point_act [4]; 0/screen [84, 84])\n",
    "#   3, #select_rect                                        (7/select_add [2]; 0/screen [84, 84]; 2/screen2 [84, 84])\n",
    "#   4, #select_control_group                               (4/control_group_act [5]; 5/control_group_id [10])\n",
    "   5, #select_unit                                        (8/select_unit_act [4]; 9/select_unit_id [500])\n",
    "#   6, #select_idle_worker                                 (10/select_worker [4])\n",
    "   7, #select_army                                        (7/select_add [2])\n",
    " #  8, #select_warp_gates                                  (7/select_add [2])\n",
    " #  9, #select_larva                                       ()\n",
    " # 10, #unload                                             (12/unload_id [500])\n",
    " # 11, #build_queue                                        (11/build_queue_id [10])\n",
    " # 12, #Attack_screen                                      (3/queued [2]; 0/screen [84, 84])\n",
    " # 13, #Attack_minimap                                     (3/queued [2]; 1/minimap [64, 64])\n",
    " 331, #Move_screen                                        (3/queued [2]; 0/screen [84, 84])\n",
    " 332 #Move_minimap                                       (3/queued [2]; 1/minimap [64, 64])\n",
    "]\n",
    "\n",
    "\n",
    "_PLAYER_SELF = features.PlayerRelative.SELF\n",
    "_PLAYER_NEUTRAL = features.PlayerRelative.NEUTRAL  # beacon/minerals\n",
    "_PLAYER_ENEMY = features.PlayerRelative.ENEMY\n",
    "\n",
    "\n",
    "#This represents the base interface for how our agent will work\n",
    "#We separate this from the StarCraft II agent class so we can focus on the underlying RL\n",
    "#implementation later...\n",
    "class Brain:\n",
    "    def __init__(self, race=\"T\", actions = default_actions):\n",
    "        self.race = race\n",
    "        self.actions = actions\n",
    "    #By default, our brain will just do nothing.\n",
    "    #We will change this later...\n",
    "    def step(self, obs):\n",
    "        return 0, []\n",
    "        \n",
    "#        if 331 in obs.observation.available_actions:\n",
    "#            player_relative = obs.observation.feature_screen.player_relative\n",
    "#            beacon = _xy_locs(player_relative == _PLAYER_NEUTRAL)\n",
    "#            if not beacon:\n",
    "#                return 0, []\n",
    "#            beacon_center = np.mean(beacon, axis=0).round()\n",
    "#            return 331, [[0], beacon_center]\n",
    "#        else:\n",
    "#            return 7, [[0]]\n",
    "\n",
    "\n",
    "#This represents the actual agent which will play StarCraft II\n",
    "class MyAgent(BaseAgent):\n",
    "    def __init__(self, brain = Brain()):\n",
    "        super().__init__() #call parent constructor\n",
    "        assert isinstance(brain, Brain)\n",
    "        self.brain = brain\n",
    "        \n",
    "    def step(self, obs): #This function is called once per frame to give the AI observation data and return its action\n",
    "        super().step(obs) #call parent base method\n",
    "        action, params = self.brain.step(obs)\n",
    "        return actions.FunctionCall(action, params)\n",
    "        \n",
    "agent = MyAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>From here, we can test our our agent by calling the following cell. The first line exports our notbook code as a Python file. The second line actually runs our agent.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#300a24\"><b><p style=\"color:white\">\n",
    "jupyter nbconvert --to script PySC2_Basics<br>python3 -m pysc2.bin.agent --map MoveToBeacon --agent PySC2_Basics.MyAgent</p></b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To scale up our training performance, we will be using the Synchronous Actor Advantage Critic (A2C) reinforcement learning algorithm, which allows us to train our agent multiple times in parallel. Starter code is provided <a href=\"https://github.com/MG2033/A2C\">here</a>. First, we will need to import some modules.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "UNIT_ELEMENTS = 7\n",
    "MAXIMUM_CARGO = 10\n",
    "MAXIMUM_BUILD_QUEUE = 10\n",
    "MAXIMUM_MULTI_SELECT = 10\n",
    "class StateNet:\n",
    "    def __init__(self, scope, nonspatial_actions = len(default_actions),\n",
    "                 resolution=84, channels=20, max_multi_select=MAXIMUM_MULTI_SELECT,\n",
    "                 max_cargo=MAXIMUM_CARGO, max_build_queue=MAXIMUM_BUILD_QUEUE,\n",
    "                 l2_scale=0.01, hidden_size=256):\n",
    "        self.resolution = resolution\n",
    "        self.variable_feature_sizes = {\n",
    "            'multi_select' : max_multi_select,\n",
    "            'cargo' :  max_cargo, \n",
    "            'build_queue' : max_build_queue\n",
    "        }\n",
    "        #The following assumes that we will stack our minimap and screen features (and they will have the same size)\n",
    "        with tf.variable_scope('State-{}'.format(scope)):\n",
    "            self.structured_observation = tf.placeholder(tf.float32, [None, 11], 'StructuredObservation')\n",
    "            self.single_select = tf.placeholder(tf.float32, [None, 1, UNIT_ELEMENTS], 'SingleSelect')\n",
    "            self.cargo = tf.placeholder(tf.float32, [None,  max_cargo, UNIT_ELEMENTS], 'Cargo')\n",
    "            self.multi_select = tf.placeholder(tf.float32, [None, max_multi_select, UNIT_ELEMENTS], 'Multiselect')\n",
    "            self.build_queue = tf.placeholder(tf.float32, [None,  max_build_queue, UNIT_ELEMENTS], 'BuildQueue')\n",
    "            self.units = tf.concat([self.single_select,\n",
    "                                    self.multi_select,\n",
    "                                    self.cargo,\n",
    "                                    self.build_queue], axis=1,\n",
    "                                    name='Units')\n",
    "            self.control_groups = tf.placeholder(tf.float32, [None, 10, 2], 'ControlGroups')\n",
    "            self.available_actions = tf.placeholder(tf.float32, [None, nonspatial_actions], 'AvailableActions')\n",
    "            self.used_actions = tf.placeholder(tf.float32, [None, nonspatial_actions], 'UsedActions')\n",
    "            self.actions = tf.concat([self.available_actions,\n",
    "                                      self.used_actions], axis=1,\n",
    "                                      name='Actions')\n",
    "            self.nonspatial_features = tf.concat([\n",
    "                    self.structured_observation,\n",
    "                    tf.reshape(self.units, [-1, UNIT_ELEMENTS * (1+sum(self.variable_feature_sizes.values()))]),\n",
    "                    tf.reshape(self.control_groups, [-1, 20]),\n",
    "                    tf.reshape(self.actions, [-1, 2 * nonspatial_actions])\n",
    "                ], axis=1, name='NonspatialFeatures')\n",
    "            \n",
    "            self.spatial_features = tf.placeholder(tf.float32,\n",
    "                                                   [None, resolution, resolution, channels],\n",
    "                                                   'SpatialFeatures')\n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.spatial_features, filters=32,\n",
    "                                          kernel_size=[5, 5],\n",
    "                                          kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_scale),\n",
    "                                          activation=tf.nn.relu, name='Convolutional1')\n",
    "            self.max_pool1 = tf.layers.max_pooling2d(inputs=self.conv1, pool_size=[2, 2],\n",
    "                                                     strides=2, name='Pool1')\n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.max_pool1, filters=64,\n",
    "                                          kernel_size=[5, 5],\n",
    "                                          kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_scale),\n",
    "                                          activation=tf.nn.relu, name='Convolutional2')\n",
    "            self.max_pool2 = tf.layers.max_pooling2d(inputs=self.conv2, pool_size=[2, 2],\n",
    "                                                     strides=2, name='Pool2')\n",
    "            self.max_pool2_flat = tf.reshape(self.max_pool2, [-1, 18 * 18 * 64], name='Pool2_Flattened')\n",
    "            self.state_flattened = tf.concat([self.max_pool2_flat, self.nonspatial_features],\n",
    "                                             1, name='StateFlattened')\n",
    "            self.hidden_1 = tf.layers.dense(self.state_flattened, hidden_size, tf.nn.relu, name='Hidden1')\n",
    "            self.output = self.hidden_1\n",
    "            for variable_name, tensor in vars(self).items():\n",
    "                if isinstance(tensor, tf.Tensor):\n",
    "                    print('{}:\\t({} Shape={})'.format(variable_name, tensor.name, tensor.shape))\n",
    "            \n",
    "tf.reset_default_graph()\n",
    "test_state = StateNet('test')\n",
    "global_state = StateNet('global')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class QNet:\n",
    "    def __init__(self, statenet, scope, usable_actions=default_actions):\n",
    "        self.actions = usable_actions\n",
    "        with tf.variable_scope(scope):\n",
    "            self.action_probability_raw = tf.layers.dense(statenet.output,\n",
    "                                                          len(self.actions),\n",
    "                                                          tf.nn.relu,\n",
    "                                                          name='ActionProbRaw')\n",
    "            self.action_probability = tf.nn.softmax(self.action_probability_raw)\n",
    "            print('Action probability shape:', self.action_probability.shape)\n",
    "            self.arguments = {}\n",
    "            for argument in actions.TYPES:\n",
    "                self.arguments[argument.name] = [] \n",
    "                for dimension, size in enumerate(argument.sizes):\n",
    "                    if size == 0: #set size for screen/minimap coordinates\n",
    "                        size = 1\n",
    "                        if argument.name in ['screen', 'screen2', 'minimap']:\n",
    "                            size = statenet.resolution\n",
    "                    argument_layer = tf.layers.dense(statenet.output, \n",
    "                                                     size, tf.nn.relu,\n",
    "                                                     name='{}{}'.format(argument.name,\n",
    "                                                                        dimension))\n",
    "                    print('Argument {}[{}] Shape:{}'.format(argument.name, dimension, argument_layer.shape))\n",
    "                    self.arguments[argument.name].append(argument_layer)\n",
    "            self.value =  tf.layers.dense(statenet.output, 1, name='Value')\n",
    "            print('{} Shape: {}'.format(self.value.name, self.value.shape))\n",
    "        \n",
    "test_q = QNet(test_state, 'test')\n",
    "global_q = QNet(global_state, 'global')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "entropy_factor = 0.1\n",
    "value_factor = 0.5\n",
    "gradient_norm_factor = 40\n",
    "default_optimizer = tf.train.AdamOptimizer(learning_rate=0.001)\n",
    "class QTrainer:\n",
    "    def __init__(self, statenet, qnet, scope, optimizer=default_optimizer):\n",
    "        with tf.variable_scope('QTrainer-{}'.format(scope)):\n",
    "            self.action = tf.placeholder(tf.int32, [None], name='Action_Placeholder')\n",
    "            self.action_one_hot = tf.one_hot(self.action, len(qnet.actions), name='Actions_One_Hot')\n",
    "            self.entropy_base = -tf.reduce_sum(qnet.action_probability * tf.log(\n",
    "                    tf.clip_by_value(qnet.action_probability, 1e-20, 1)\n",
    "                ), [1]\n",
    "            )\n",
    "            self.arguments = {} #placeholders for argument training\n",
    "            self.arguments_one_hot = {}\n",
    "            self.arguments_entropy = {}\n",
    "            for argument in actions.TYPES:\n",
    "                self.arguments[argument.name] = [] \n",
    "                self.arguments_one_hot[argument.name] =[]\n",
    "                self.arguments_entropy[argument.name] = []\n",
    "                for dimension, size in enumerate(argument.sizes):\n",
    "                    argument_placeholder = tf.placeholder(tf.int32,[None],\n",
    "                                                          name='{}{}_Placeholder'.format(argument.name,\n",
    "                                                                                         dimension))\n",
    "                    self.arguments[argument.name].append(argument_placeholder)\n",
    "                    argument_one_hot = tf.one_hot(argument_placeholder,\n",
    "                                                  qnet.arguments[argument.name][dimension].shape[1],\n",
    "                                                  dtype=tf.float32,\n",
    "                                                  name='{}{}_One_Hot'.format(argument.name,\n",
    "                                                                                 dimension))\n",
    "                    self.arguments_one_hot[argument.name].append(argument_one_hot)\n",
    "                    print('{} Shape:{}'.format(argument_one_hot.name,\n",
    "                                                        argument_one_hot.shape))\n",
    "                    \n",
    "                    argument_entropy = -tf.reduce_sum(\n",
    "                        qnet.arguments[argument.name][dimension] * tf.log(\n",
    "                            tf.clip_by_value(qnet.arguments[argument.name][dimension], 1e-20, 1)\n",
    "                        ), [1]\n",
    "                    )\n",
    "                    self.arguments_entropy[argument.name].append(argument_entropy)\n",
    "                    self.entropy_base += argument_entropy\n",
    "            self.target_value = tf.placeholder(tf.float32, shape=[None], name='Target_Value')\n",
    "            #Multiplying our QNet's action probabilities by the one hot action tensor\n",
    "            self.value_loss = qnet.value - self.target_value\n",
    "            self.loss = self.value_loss * value_factor + self.entropy_base * entropy_factor\n",
    "            local_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)\n",
    "            self.gradients = tf.gradients(self.loss, local_vars)\n",
    "            self.var_norms = tf.global_norm(local_vars)\n",
    "            grads, self.grad_norms = tf.clip_by_global_norm(self.gradients, gradient_norm_factor)\n",
    "\n",
    "            # Apply local gradients to global network\n",
    "            global_vars = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope)#'global')\n",
    "            self.apply_grads = optimizer.apply_gradients(zip(grads,global_vars))\n",
    "            \n",
    "test_qtrainer = QTrainer(test_state, test_q, 'test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RLBrain(Brain):\n",
    "    def __init__(self, name, sess=None, race=\"T\", actions = default_actions):\n",
    "        super().__init__()\n",
    "        self.state = StateNet(name, nonspatial_actions=len(actions))\n",
    "        self.q = QNet(self.state, name, usable_actions=actions)\n",
    "        self.q_trainer = QTrainer(self.state, self.q, name)\n",
    "        self.sess = sess\n",
    "    #By default, our brain will just do nothing.\n",
    "    #We will change this later...\n",
    "    def step(self, obs):\n",
    "        #formatting/processing our observation to feed into state/q nets\n",
    "        #determine our action probabilities\n",
    "        #select action randomly with probabilities\n",
    "        #for each required  argument:\n",
    "            #determine argument value probabilities\n",
    "            #select argument value randomly with probabilities\n",
    "        #return action id, arguments\n",
    "        return 0, []\n",
    "    \n",
    "    def process_observations(self, observation):\n",
    "        # is episode over?\n",
    "        episode_end = (observation.step_type == environment.StepType.LAST)\n",
    "        # reward\n",
    "        reward = observation.reward #scalar?\n",
    "        # features\n",
    "        features = observation.observation\n",
    "        # the shapes of some features depend on the state (eg. shape of multi_select depends on number of units)\n",
    "        # since tf requires fixed input shapes, we set a maximum size then pad the input if it falls short\n",
    "        processed_features = {}\n",
    "        for feature_label, feature in observation.observation.items():\n",
    "            if feature_label in ['available_actions', 'last_actions']:\n",
    "                actions = np.zeros(len(self.actions))\n",
    "                for i, action in enumerate(self.actions):\n",
    "                    if action in feature:\n",
    "                        actions[i] = 1\n",
    "                feature = actions\n",
    "            if feature_label in ['minimap', 'screen']:\n",
    "                feature = np.stack(feature, axis=2)\n",
    "            elif feature_label in ['single_select', 'multi_select', 'cargo', 'build_queue']:\n",
    "                if feature_label in self.state.variable_feature_sizes:\n",
    "                    padding = np.zeros((self.state.variable_feature_sizes[feature_label] - len(feature), UNIT_ELEMENTS))\n",
    "                    feature = np.concatenate(feature, padding)\n",
    "                feature = feature.reshape(-1, UNIT_ELEMENTS)\n",
    "            elif feature_label == 'structured':\n",
    "                \n",
    "            else:\n",
    "                continue\n",
    "            processed_features[feature_label] = np.expand_dims(feature, axis=0)\n",
    "        return reward, processed_features, episode_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "brain = RLBrain('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet:\n",
    "    def __init__(self, scope, statenet, resolution=84):\n",
    "        self.statenet = statenet\n",
    "        self.input = statenet.output\n",
    "        self.hidden_layer = tf.layers.dense(self.input, 256, activation=tf.nn.relu)\n",
    "        self.raw_output = tf.layers.dense(self.hidden_layer, 2)\n",
    "        self.clipped_output = tf.clip_by_value(self.raw_output, 0, resolution-1)\n",
    "        self.output = tf.cast(self.clipped_output, tf.int32)\n",
    "        for variable_name, tensor in vars(self).items():\n",
    "            if isinstance(tensor, tf.Tensor):\n",
    "                print('{}:\\t({} Shape={})'.format(variable_name, tensor.name, tensor.shape))\n",
    "        \n",
    "        \n",
    "        \n",
    "class SimpleBrain(Brain):\n",
    "    def  __init__(self, scope, race=\"T\", actions = default_actions):\n",
    "        super().__init__(race, actions)\n",
    "        self.statenet = StateNet(scope)\n",
    "        self.simplenet = SimpleNet(scope, self.statenet)\n",
    "        self.sess = tf.Session()\n",
    "        self.feature_placeholders = {\n",
    "            'available_actions' : self.statenet.available_actions,\n",
    "            'last_actions' : self.statenet.used_actions,\n",
    "            'cargo' : self.statenet.cargo,\n",
    "            'multi_select' : self.statenet.multi_select,\n",
    "            'single_select' : self.statenet.single_select,\n",
    "            'build_queue' : self.statenet.build_queue,\n",
    "            'structured' : self.statenet.structured_observation,\n",
    "            'control_groups' : self.statenet.control_groups,\n",
    "            'multi_select' : self.statenet.multi_select,\n",
    "        }\n",
    "        \n",
    "        \n",
    "    def step(self, obs):\n",
    "        if 331 in obs.observations.available_actions:\n",
    "            reward, feed_dict, episode_end = self.proccess_observations(obs)\n",
    "            outputs = sess.run(simplenet.output, feed_dict)\n",
    "            \n",
    "    def process_observations(self, observation):\n",
    "        # is episode over?\n",
    "        episode_end = (observation.step_type == environment.StepType.LAST)\n",
    "        # reward\n",
    "        reward = observation.reward #scalar?\n",
    "        # features\n",
    "        features = observation.observation\n",
    "        # the shapes of some features depend on the state (eg. shape of multi_select depends on number of units)\n",
    "        # since tf requires fixed input shapes, we set a maximum size then pad the input if it falls short\n",
    "        processed_features = {}\n",
    "        for feature_label, feature in observation.observation.items():\n",
    "            if feature_label in ['available_actions', 'last_actions']:\n",
    "                actions = np.zeros(len(self.actions))\n",
    "                for i, action in enumerate(self.actions):\n",
    "                    if action in feature:\n",
    "                        actions[i] = 1\n",
    "                feature = actions\n",
    "            if feature_label in ['minimap', 'screen']:\n",
    "                feature = np.stack(feature, axis=2)\n",
    "            elif feature_label in ['single_select', 'multi_select', 'cargo', 'build_queue']:\n",
    "                if feature_label in self.state.variable_feature_sizes:\n",
    "                    padding = np.zeros((self.state.variable_feature_sizes[feature_label] - len(feature), UNIT_ELEMENTS))\n",
    "                    feature = np.concatenate(feature, padding)\n",
    "                feature = feature.reshape(-1, UNIT_ELEMENTS)\n",
    "            elif feature_label == 'structured':\n",
    "                pass\n",
    "            else:\n",
    "                continue\n",
    "            placeholder = self.feature_placeholders[feature_label]\n",
    "            processed_features[placeholder] = np.expand_dims(feature, axis=0)\n",
    "        return reward, processed_features, episode_end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pysc2.bin.agent import main\n",
    "from absl import app\n",
    "import tensorflow as tf\n",
    "tf.reset_default_graph()\n",
    "\n",
    "#point_flag.DEFINE_point(\"feature_screen_size\", \"84\",\n",
    "#                        \"Resolution for screen feature layers.\")\n",
    "#point_flag.DEFINE_point(\"feature_minimap_size\", \"64\",\n",
    "#\"Resolution for minimap feature layers.\")\n",
    "args = [ None, '--map', 'MoveToBeacon', '--agent=MyAgent.SimpleAgent', '--feature_minimap_size', '84']\n",
    "app.run(main, args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
