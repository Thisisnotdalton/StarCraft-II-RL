{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Getting Started with PySC2</h1>\n",
    "<p>For additional information, please refer to the official GitHub repository linked <a href=\"https://github.com/deepmind/pysc2\">here</a>.</p>\n",
    "<p>To get started with PySC2, first verify that you have the proper Python packages installed as well as the mini game maps linked in the PySC2 repository!</p>\n",
    "<p>Please run the following cell to make sure that your computer has all of the proper software setup.</p>\n",
    "<p>If you have everything installed properly, the running the following cell should open an instance of StarCraft with a randomly acting agent playing on the MoveToBeacon map:</p>\n",
    "<div style=\"background-color:#300a24\"><b><p style=\"color:white\">python3 -m pysc2.bin.agent --map MoveToBeacon --agent pysc2.agents.random_agent.RandomAgent</p></b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>Assuming all of that worked, we will continue by setting up some of the basic configuration for our StarCraft AI. In order to make our AI, we will create a Python class to represent our StarCraft agent. We will make our class inherit from the base agent which PySC2 provides.</p>\n",
    "\n",
    "<p>Some things worth noting are that we define a list of default actions for our agent to use. This will be used to restrict which actions our agent is allowed to perform. To view the list of all valid actions our agent can perform, try entering the following command in a terminal:</p>\n",
    "<div style=\"background-color:#300a24\"><b><p style=\"color:white\">python3 -m pysc2.bin.valid_actions --hide_specific</p></b></div>\n",
    "\n",
    "\n",
    "<p>Running this command produces many lines of output giving you the numerical id's of various StarCraft actions our agent can perform. A smaller subset of actions has been selected with comments with their name so that our agent is not slowed down with too many actions to learn.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pysc2.agents.base_agent import BaseAgent\n",
    "from pysc2.lib import actions\n",
    "\n",
    "default_actions = [\n",
    "   0, #no_op                                              ()\n",
    "   1, #move_camera                                        (1/minimap [64, 64])\n",
    "   2, #select_point                                       (6/select_point_act [4]; 0/screen [84, 84])\n",
    "   3, #select_rect                                        (7/select_add [2]; 0/screen [84, 84]; 2/screen2 [84, 84])\n",
    "   4, #select_control_group                               (4/control_group_act [5]; 5/control_group_id [10])\n",
    "   5, #select_unit                                        (8/select_unit_act [4]; 9/select_unit_id [500])\n",
    "   6, #select_idle_worker                                 (10/select_worker [4])\n",
    "   7, #select_army                                        (7/select_add [2])\n",
    "   8, #select_warp_gates                                  (7/select_add [2])\n",
    "   9, #select_larva                                       ()\n",
    "  10, #unload                                             (12/unload_id [500])\n",
    "  11, #build_queue                                        (11/build_queue_id [10])\n",
    "  12, #Attack_screen                                      (3/queued [2]; 0/screen [84, 84])\n",
    "  13 #Attack_minimap                                     (3/queued [2]; 1/minimap [64, 64])\n",
    "]\n",
    "\n",
    "#This represents the base interface for how our agent will work\n",
    "#We separate this from the StarCraft II agent class so we can focus on the underlying RL\n",
    "#implementation later...\n",
    "class Brain:\n",
    "    def __init__(self, race=\"T\", actions = default_actions):\n",
    "        self.race = race\n",
    "        self.actions = actions\n",
    "    \n",
    "    #By default, our brain will just do nothing.\n",
    "    #We will change this later...\n",
    "    def step(self, obs):\n",
    "        return 0, []\n",
    "\n",
    "\n",
    "#This represents the actual agent which will play StarCraft II\n",
    "class MyAgent(BaseAgent):\n",
    "    def __init__(self, brain = Brain()):\n",
    "        super().__init__() #call parent constructor\n",
    "        assert isinstance(brain, Brain)\n",
    "        self.brain = brain\n",
    "        \n",
    "    def step(self, obs): #This function is called once per frame to give the AI observation data and return its action\n",
    "        super().step(obs) #call parent base method\n",
    "        action, params = self.brain.step(obs)\n",
    "        return actions.FunctionCall(action, params)\n",
    "        \n",
    "agent = MyAgent()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>From here, we can test our our agent by calling the following cell. The first line exports our notbook code as a Python file. The second line actually runs our agent.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"background-color:#300a24\"><b><p style=\"color:white\">\n",
    "jupyter nbconvert --to script PySC2_Basics<br>python3 -m pysc2.bin.agent --map MoveToBeacon --agent PySC2_Basics.MyAgent</p></b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>To scale up our training performance, we will be using the Synchronous Actor Advantage Critic (A2C) reinforcement learning algorithm, which allows us to train our agent multiple times in parallel. Starter code is provided <a href=\"https://github.com/MG2033/A2C\">here</a>. First, we will need to import some modules.<p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Units shape: (?, 31, 7)\n",
      "Actions shape: (?, 20)\n",
      "Nonspatial features shape: (?, 268)\n",
      "Convolutional1 Shape: (?, 80, 80, 32)\n",
      "Pool Shape1: (?, 40, 40, 32)\n",
      "Convolutional2 Shape: (?, 36, 36, 64)\n",
      "Pool Shape2: (?, 18, 18, 64)\n",
      "Pool flattened shape: (?, 20736)\n",
      "State flat shape: (?, 21004)\n"
     ]
    }
   ],
   "source": [
    "default_actor_optimizer = tf.train.RMSPropOptimizer(0.0001, name='RMSPropA')\n",
    "default_critic_optimizer = tf.train.RMSPropOptimizer(0.001, name='RMSPropC')\n",
    "UNIT_ELEMENTS = 7\n",
    "ENTROPY_BETA = 0.01\n",
    "default_nonspatial_spec = 10\n",
    "\n",
    "class StateNet:\n",
    "    def __init__(self, scope, nonspatial_actions,\n",
    "                 resolution=84, channels=20, max_multi_select=10,\n",
    "                 max_cargo=10, max_build_queue=10, l2_scale=0.01, hidden_size=256, lstm_size=256):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.structured_observation = tf.placeholder(tf.float32, [None, 11], 'StructuredObservation')\n",
    "            self.single_select = tf.placeholder(tf.float32, [None, 1, UNIT_ELEMENTS], 'SingleSelect')\n",
    "            self.cargo = tf.placeholder(tf.float32, [None,  max_cargo, UNIT_ELEMENTS], 'Cargo')\n",
    "            self.multi_select = tf.placeholder(tf.float32, [None, max_multi_select, UNIT_ELEMENTS], 'Multiselect')\n",
    "            self.build_queue = tf.placeholder(tf.float32, [None,  max_build_queue, UNIT_ELEMENTS], 'BuildQueue')\n",
    "            self.units = tf.concat([self.single_select,\n",
    "                                    self.multi_select,\n",
    "                                    self.cargo,\n",
    "                                    self.build_queue], axis=1)\n",
    "            print('Units shape:',self.units.shape)\n",
    "            self.control_groups = tf.placeholder(tf.float32, [None, 10, 2], 'ControlGroups')\n",
    "            self.available_actions = tf.placeholder(tf.float32, [None, nonspatial_actions], 'AvailableActions')\n",
    "            self.used_actions = tf.placeholder(tf.float32, [None, nonspatial_actions], 'UsedActions')\n",
    "            self.actions = tf.concat([self.available_actions,\n",
    "                                      self.used_actions], axis=1)\n",
    "            print('Actions shape:', self.actions.shape)\n",
    "            self.nonspatial_features = tf.concat([\n",
    "                    self.structured_observation,\n",
    "                    tf.reshape(self.units, [-1, UNIT_ELEMENTS * (1+max_multi_select+max_build_queue+max_cargo)]),\n",
    "                    tf.reshape(self.control_groups, [-1, 10*2]),\n",
    "                    tf.reshape(self.actions, [-1, 2 * nonspatial_actions])\n",
    "                ], axis=1)\n",
    "            \n",
    "            print('Nonspatial features shape:',self.nonspatial_features.shape)\n",
    "            \n",
    "            self.spatial_features = tf.placeholder(tf.float32, [None, resolution, resolution, channels], 'SpatialFeatures')\n",
    "            self.conv1 = tf.layers.conv2d(inputs=self.spatial_features, filters=32,\n",
    "                                          kernel_size=[5, 5],\n",
    "                                          kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_scale),\n",
    "                                          activation=tf.nn.relu, name='Convolutional1')\n",
    "            print('Convolutional1 Shape:',self.conv1.shape)\n",
    "            self.max_pool1 = tf.layers.max_pooling2d(inputs=self.conv1, pool_size=[2, 2], strides=2, name='Pool1')\n",
    "            print('Pool Shape1:',self.max_pool1.shape)\n",
    "            self.conv2 = tf.layers.conv2d(inputs=self.max_pool1, filters=64,\n",
    "                                          kernel_size=[5, 5],\n",
    "                                          kernel_regularizer=tf.contrib.layers.l2_regularizer(l2_scale),\n",
    "                                          activation=tf.nn.relu, name='Convolutional2')\n",
    "            print('Convolutional2 Shape:',self.conv2.shape)\n",
    "            self.max_pool2 = tf.layers.max_pooling2d(inputs=self.conv2, pool_size=[2, 2], strides=2, name='Pool2')\n",
    "            print('Pool Shape2:',self.max_pool2.shape)\n",
    "            self.max_pool2_flat = tf.reshape(self.max_pool2, [-1, 18 * 18 * 64])\n",
    "            print('Pool flattened shape:', self.max_pool2_flat.shape)\n",
    "            self.state_flattened = tf.concat([self.max_pool2_flat, self.nonspatial_features], 1, name='StateFlattened')\n",
    "            print('State flat shape:', self.state_flattened.shape)\n",
    "            self.hidden_1 = tf.layers.dense(self.state_flattened, hidden_size, tf.nn.relu, name='Hidden1')\n",
    "\n",
    "tf.reset_default_graph()\n",
    "test_state = StateNet('global', default_nonspatial_spec)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ACNet:\n",
    "    def __init__(self, scope, state_spec = default_state_spec,\n",
    "                 nonspatial_spec = default_nonspatial_spec,\n",
    "                 spatial_spec = default_spatial_spec,\n",
    "                 global_ACNet=None,\n",
    "                 actor_optimizer=default_actor_optimizer,\n",
    "                 critic_optimizer=default_critic_optimizer):\n",
    "        with tf.variable_scope(scope):\n",
    "            self.state = tf.placeholder(tf.float32, [None]+[i for i in state_spec], 'State')    \n",
    "            self.build_actor_critic(scope, nonspatial_spec, resolution)\n",
    "            if global_ACNet is not None:# We have a global net, making a worker net\n",
    "                self.nonspatial_action_history = tf.placeholder(tf.float32, [None, nonspatial_spec], 'Action')        # action\n",
    "                self.v_target = tf.placeholder(tf.float32, [None, 1], 'Vtarget') # v_target value\n",
    "                td = tf.subtract(self.v_target, self.v, name='TD_error')\n",
    "                with tf.name_scope('critic_loss'):\n",
    "                    self.critic_loss = tf.reduce_mean(tf.square(td))\n",
    "\n",
    "                normal_dist = {}\n",
    "                log_prob = {}\n",
    "                self.exp_v = {}\n",
    "                self.action_loss = 0\n",
    "                for dim in self.mu:\n",
    "                    with tf.name_scope('wrap_a_out'):\n",
    "                        self.mu[dim], self.sigma[dim] = self.mu[dim] * spatial_spec[1], self.sigma[dim] + 1e-4\n",
    "                    normal_dist[dim] = tf.contrib.distributions.Normal(self.mu[dim], self.sigma[dim])\n",
    "    \n",
    "                    with tf.name_scope('a_loss'):\n",
    "                        log_prob[dim] = normal_dist[dim].log_prob(self.nonspatial_action_history)\n",
    "                        exp_v = log_prob[dim] * td\n",
    "                        entropy = normal_dist[dim].entropy()  # encourage exploration\n",
    "                        self.exp_v = ENTROPY_BETA * entropy + exp_v\n",
    "                        self.action_loss -= self.exp_v[dim]\n",
    "    \n",
    "    def build_actor_critic(self, scope, nonspatial_spec, spatial_spec, actor_units=200, critic_units=100):\n",
    "        w_init = tf.random_normal_initializer(0., .1)\n",
    "        with tf.variable_scope('actor'):\n",
    "            actor_layer = tf.layers.dense(self.state, actor_units, tf.nn.relu6, kernel_initializer=w_init, name='actorlayer')\n",
    "            # estimated action value\n",
    "            self.mu = {}\n",
    "            # estimated variance\n",
    "            self.sigma = {}\n",
    "            for dim in ['x', 'y', 'nonspatial']:\n",
    "                with tf.variable_scope(dim):\n",
    "                    spec = nonspatial_spec if dim == 'nonspatial' else spatial_spec\n",
    "                    self.mu[dim] = tf.layers.dense(actor_layer, spec, tf.nn.tanh, kernel_initializer=w_init, name='mu')\n",
    "                    self.sigma[dim] = tf.layers.dense(actor_layer, spec, tf.nn.softplus, kernel_initializer=w_init, name='sigma')\n",
    "            \n",
    "        with tf.variable_scope('critic'):\n",
    "            critic_layer = tf.layers.dense(self.state, critic_units, tf.nn.relu6, kernel_initializer=w_init, name='criticlayer')\n",
    "            self.v = tf.layers.dense(critic_layer, 1, kernel_initializer=w_init, name='v')  # estimated value for state\n",
    "        self.nonspatial_actor_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/actor')\n",
    "        self.critic_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=scope + '/critic')\n",
    "        \n",
    "        \n",
    "        \n",
    "tf.reset_default_graph()\n",
    "test_global = ACNet('global_ac')\n",
    "test_worker = ACNet('worker0', global_ACNet=test_global)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
